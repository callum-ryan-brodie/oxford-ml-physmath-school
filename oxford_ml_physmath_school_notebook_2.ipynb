{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Introduction / setup\n",
        "\n",
        "This notebook is accompanying material for the 1st tutorial on Topic 2 in the **Machine Learning in Mathematics & Theoretical Physics** Summer School in Oxford in 2023.\n",
        "\n",
        "This tutorial is on the topic of reinforcement learning. We'll look at some of the basic concepts of environments, agents, and reinforcement learning algorithms. (Most of this will happen on the blackboard.)\n",
        "\n",
        "In the later parts of the tutorial we will train an agent to solve some simple games, which are given as **OpenAI gym** environments, and we will use the **Stable Baselines** library of reinforcement learning algorithms.\n",
        "\n",
        "For example, we will train a Lunar Lander agent, to land a spaceship as in the animation below:\n",
        "\n",
        "![Lunar Lander](https://cdn-images-1.medium.com/max/960/1*f4VZPKOI0PYNWiwt0la0Rg.gif)\n",
        "\n",
        "We will start the tutorial with some blackboard review on the basics of an environment, using the example of a **GridWorld** maze. While we're doing this, you can run the following code cells to set up all the requisite packages that we'll need in the notebook below (this will take a few minutes).\n",
        "\n",
        "*Note: This notebook is designed to work in a ***Google Colab*** environment. If you're running things locally, you might have to tweak some parts. Note in particular that when we visualise our agent's performance, we'll be using a virtual display, since the remote machine doesn't have a display.*"
      ],
      "metadata": {
        "id": "Z7gnVjFebvjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For some environments we will need box2d-py\n",
        "!apt-get update && apt-get install swig cmake\n",
        "!pip install box2d-py\n",
        "\n",
        "# stable-baselines3 is a library of various reinforcement learning algorithms\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "\n",
        "# For the custom GridWorld environment, we need LaTeX for some fonts\n",
        "!apt install texlive texlive-latex-extra texlive-fonts-recommended dvipng cm-super\n",
        "!pip install latex\n",
        "\n",
        "# For grabbing some files we'll need for GridWorld\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "205Q56-2VycY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the above packages installed, you should be able to import everything you need like this:"
      ],
      "metadata": {
        "id": "_bUQ0mZmPPb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import gym\n",
        "#from gym import spaces\n",
        "#from gym.utils import seeding\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import DQN, A2C\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import imageio\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "rc('text', usetex=True)\n",
        "from __future__ import print_function\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "bzuIiEbJXiD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# A basic environment: GridWorld\n",
        "\n",
        "Here we'll set up an example environment, called a **GridWorld** environment, in which an agent can move around on a grid, in which there are a number of pits into in which it can fall (bad), and there is also an exit (good).\n",
        "\n",
        "To do this we need to first grab the files ```gridworld.py``` and ```helperFunctions.py```, which we can do as follows:\n",
        "\n",
        "(You should then be able to see the files (on Google Colab) by opening the files tab on the left, by clicking the folder icon.)"
      ],
      "metadata": {
        "id": "_LIs2d9obd_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/callum-ryan-brodie/oxford-ml-physmath-school/main/gridworld.py'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('gridworld.py', 'wb').write(r.content);\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/callum-ryan-brodie/oxford-ml-physmath-school/main/helperFunctions.py'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('helperFunctions.py', 'wb').write(r.content);"
      ],
      "metadata": {
        "id": "8XQhJceklEet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then import them as:"
      ],
      "metadata": {
        "id": "M2lSPlUblbCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gridworld\n",
        "import helperFunctions"
      ],
      "metadata": {
        "id": "pEjXTOhPWCku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a **GridWorld** environment:"
      ],
      "metadata": {
        "id": "S9fqq73_ldl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(4) # Ensures the same maze each time - delete and run the below twice for a random maze\n",
        "env = gridworld.GameEnv()"
      ],
      "metadata": {
        "id": "Wa1nU9kPYCWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the initial environment by visualising it:"
      ],
      "metadata": {
        "id": "sVd7V_evZCqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "world = env.render_world()\n",
        "plt.ioff()\n",
        "plt.tight_layout()\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(world, interpolation=\"nearest\")\n",
        "plt.draw()\n",
        "plt.title(\"Maze layout\", fontsize=24)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "PlPE-Vy-X_e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the visualisation of the environment we have the following colour-coding:\n",
        "\n",
        "- In blue is the agent, which begins in the top-left.\n",
        "- In red are the pits, which we'll consider as terminal states, meaning the game ends if the agent steps into one.\n",
        "- In green is the exit of the maze, which is also a terminal state (but one we'll later encode as a 'good one' with a positive reward!)\n",
        "- (And in white are the spaces where the agent can freely walk.)\n",
        "\n",
        "The action space for the agent is discrete and consists of four possible actions: up, down, left, and right.\n",
        "\n",
        "We can step our agent through the environment by running ```env.step(i)```, where $i \\in \\{0,1,2,3\\}$ and where the index encodes the actions in the order up, down, left, right.\n",
        "\n",
        "**Q: Complete the following code cell to make the agent take a step down, and then visualise the new environment.**\n",
        "\n",
        "(Note: Make sure you don't reset the environment again before visualising!)"
      ],
      "metadata": {
        "id": "MiUf7EMnYbAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### COMPLETE THIS CODE CELL ###"
      ],
      "metadata": {
        "id": "pET5amvKYUfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: Next use the following code cell to have a play around with moving the agent into for example the pits, or outside the boundaries of the maze, and visualise the environment afterwards, to see what happens.**\n",
        "\n",
        "(And remember you can reset the environment at any point with ```env.reset()```.)"
      ],
      "metadata": {
        "id": "vdV7IoeJa_Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### COMPLETE THIS CODE CELL ###"
      ],
      "metadata": {
        "id": "PMq5S7bTbR9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, note that when we run ```env.step(i)```, it returns a tuple of outputs, respectively containing:\n",
        "1. The position $(x,y)$ that the agent ends up at after the action.\n",
        "2. The reward $r(s,a)$ for taking this action from this state.\n",
        "3. A boolean which is **True** if the agent has ended up in a terminal state.\n",
        "\n",
        "**Q: In the following code cell, play around with moving the agent with ```env.step(i)``` and look at the returned tuples, while comparing with visualisations of the environment, to see that it behaves correctly at the various terminal states, and to see what the rewards are for the various state/action possibilities.**\n",
        "\n",
        "The outputs of ```env.step(i)``` are what we will feed back to the reinforcement learning algorithm during training."
      ],
      "metadata": {
        "id": "Sa9PNn5_hSOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### COMPLETE THIS CODE CELL ###"
      ],
      "metadata": {
        "id": "mrtpPlgWhRt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Code behind an environment\n",
        "\n",
        "(In the below we have not shown some comments. And we've used ellipses to skip over code that isn't conceptually important.)\n",
        "\n",
        "We also skip over all the code that is simply for visualising the environment.\n",
        "\n",
        "\n",
        "Create a class (Python object oriented programming)\n",
        "\n",
        "```\n",
        "class GameEnv:\n",
        "    def __init__(self):\n",
        "```\n",
        "\n",
        "## State and action space\n",
        "\n",
        "The state and action space are simply set up as follows:\n",
        "```\n",
        "        self.sizeX = 5\n",
        "        self.sizeY = 5\n",
        "\n",
        "        ...\n",
        "\n",
        "        self.state = ()\n",
        "\n",
        "        ...\n",
        "\n",
        "        self.action_space = [0, 1, 2, 3]\n",
        "```\n",
        "Here the ```state``` is just the $(x,y)$ coordinate of the 'worker' (the name for the blue block being moved through the maze). Note that there is no other information in the state space - the worker's position is the only aspect of the environment that changes upon an action.\n",
        "\n",
        "Note that more generally one can have:\n",
        "- State $\\neq$ worker's state\n",
        "- Agent $\\neq$ worker\n",
        "- State $\\neq$ 'observation'\n",
        "\n",
        "\n",
        "Note that in an OpenAI gym environment (e.g. **FrozenLake** which is a type of GridWorld):\n",
        "```\n",
        "        self.observation_space = spaces.Discrete(nS)\n",
        "        self.action_space = spaces.Discrete(nA)\n",
        "```\n",
        "In the above ```nA``` and ```nS``` are the number of possible actions and states respectively.\n",
        "\n",
        "There is also for example ```spaces.Box``` for continuous state / action spaces. See the gym documentation on the ```spaces``` [superclass](https://www.gymlibrary.dev/api/spaces/) for the full set of options.\n",
        "\n",
        "## Rewards\n",
        "```\n",
        "        self.step_penalty = -1.\n",
        "        self.pitfall_penalty = -50.\n",
        "        self.exit_reward = 100000.\n",
        "        self.no_move_penalty = -2.\n",
        "```\n",
        "\n",
        "## Initialisation\n",
        "\n",
        "```\n",
        "class GameOb:\n",
        "    def __init__(self, name, reward, coordinates, size, rgba):\n",
        "        self.x = coordinates[0]\n",
        "        self.y = coordinates[1]\n",
        "        self.size = size\n",
        "        self.channel = rgba\n",
        "        self.reward = reward\n",
        "        self.name = name\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "    self.num_pits = 7\n",
        "\n",
        "    ...\n",
        "\n",
        "    self.objects = []\n",
        "\n",
        "    ...\n",
        "\n",
        "    self.initial_x = 0\n",
        "    self.initial_y = 0\n",
        "\n",
        "    ...\n",
        "\n",
        "    def initialize_world(self):\n",
        "        self.objects = []\n",
        "\n",
        "        maze_exit = GameOb('exit', self.exit_reward, [4, 4], 1, [0, 1, 0, 1])\n",
        "        self.objects.append(maze_exit)\n",
        "\n",
        "        worker = GameOb('worker', None, [0, 0], 1, [0, 0, 1, 1])\n",
        "        self.objects.append(worker)\n",
        "        for i in range(self.num_pits):  # add pitfalls\n",
        "            pitfall = GameOb('pitfall', self.pitfall_penalty, self.new_position(), 1, [1, 0, 0, 1])\n",
        "            self.objects.append(pitfall)\n",
        "\n",
        "        ...\n",
        "```\n",
        "\n",
        "## Resetting the environment\n",
        "\n",
        "We reset the environment by running ```env.reset()```:\n",
        "```\n",
        "    def reset(self):\n",
        "        self.steps = 0\n",
        "        self.steps_taken = []\n",
        "        self.gave_up = False\n",
        "        self.fell = False\n",
        "        self.state = (self.initial_x, self.initial_y)\n",
        "\n",
        "        for obj in self.objects:\n",
        "            if obj.name == 'worker':\n",
        "                obj.x = self.initial_x\n",
        "                obj.y = self.initial_y\n",
        "                break\n",
        "```\n",
        "\n",
        "## Taking an action\n",
        "\n",
        "We take an action by running ```env.step()```:\n",
        "\n",
        "```\n",
        "\n",
        "    self.max_steps = 1000\n",
        "\n",
        "    ...\n",
        "\n",
        "    def step(self, action, update_view=True):\n",
        "\n",
        "        ...\n",
        "\n",
        "        reward, done = self.move_worker(action)\n",
        "\n",
        "        self.steps += 1\n",
        "        self.steps_taken.append(action)\n",
        "\n",
        "        if self.steps >= self.max_steps and not done:\n",
        "            done = True\n",
        "            self.gave_up = True\n",
        "\n",
        "        if self.fell:\n",
        "            done = True\n",
        "\n",
        "        ...\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "```\n",
        "\n",
        "We see that the actual environment update is handled in the separate function ```move_worker```:\n",
        "\n",
        "```\n",
        "    def move_worker(self, direction):\n",
        "\n",
        "        worker = None\n",
        "        others = []\n",
        "        for obj in self.objects:\n",
        "            if obj.name == 'worker':\n",
        "                worker = obj\n",
        "            else:\n",
        "                others.append(obj)\n",
        "\n",
        "        worker_x = worker.x\n",
        "        worker_y = worker.y\n",
        "\n",
        "        reward = self.step_penalty\n",
        "\n",
        "        if direction == 0 and worker.y >= 1:\n",
        "            worker.y -= 1\n",
        "        if direction == 1 and worker.y <= self.sizeY - 2:\n",
        "            worker.y += 1\n",
        "        if direction == 2 and worker.x >= 1:\n",
        "            worker.x -= 1\n",
        "        if direction == 3 and worker.x <= self.sizeX - 2:\n",
        "            worker.x += 1\n",
        "\n",
        "        if worker.x == worker_x and worker.y == worker_y:\n",
        "            reward = self.no_move_penalty\n",
        "\n",
        "        for i in range(len(self.objects)):\n",
        "            if self.objects[i].name == 'worker':\n",
        "                self.objects[i] = worker\n",
        "                break\n",
        "\n",
        "        is_maze_solved = False\n",
        "        for other in others:\n",
        "            if worker.x == other.x and worker.y == other.y:\n",
        "                if other.name == \"exit\":\n",
        "                    is_maze_solved = True\n",
        "                    reward = other.reward\n",
        "                    break\n",
        "                elif other.name == \"pitfall\":\n",
        "                    is_maze_solved = False\n",
        "                    reward = other.reward\n",
        "                    self.fell = True\n",
        "                    break\n",
        "\n",
        "        return reward, is_maze_solved\n",
        "```"
      ],
      "metadata": {
        "id": "vKtsLrTIhS4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# The LunarLander environment, with a Deep Q-Network\n",
        "\n",
        "Here we'll look at utilising a standard reinforcement learning algorithm, called DQN, which we'll get from the **Stable Baselines** library (see [here](https://stable-baselines3.readthedocs.io/en/v2.0.0/modules/dqn.html)).\n",
        "\n",
        "To see the utility of this algorithm we'll look at a more complicated environment than the **GridWorld** environment above, namely the [**Lunar Lander**](https://www.gymlibrary.dev/environments/box2d/lunar_lander/) environment that comes with OpenAI's **gym**.\n",
        "\n",
        "As one of the standard **gym** environments, we can set up an agent for this environment simply as:"
      ],
      "metadata": {
        "id": "jo01XPfOPNlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    \"LunarLander-v2\",\n",
        "    verbose=1,\n",
        "    exploration_final_eps=0.1, # The lowest epsilon value to reach (for explore vs. exploit.)\n",
        "    target_update_interval=250, # How often to update the network\n",
        ")"
      ],
      "metadata": {
        "id": "BbSWDW25Pd5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently our agent is untrained, so if we evaluate it we will see that it doesn't perform well.\n",
        "\n",
        "Here we create a separate environment in which to evaluate the agent.\n",
        "\n",
        "(We'll see below that a good performance is something like a $+200$ reward.)"
      ],
      "metadata": {
        "id": "hc9lqBOORO5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate env for evaluation\n",
        "eval_env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model,\n",
        "    eval_env,\n",
        "    n_eval_episodes=10,\n",
        "    deterministic=True,\n",
        ")\n",
        "\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "LAwJ4kk5PieH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The syntax to train this model is also very simple:\n",
        "\n",
        "(Here I'll train it for 10,000 timesteps, but this won't be enough to get good performance. Try training it for 100,000 timesteps instead.)"
      ],
      "metadata": {
        "id": "jGqLB1cQW2Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=int(1e4))"
      ],
      "metadata": {
        "id": "S_GcUHT8Pmsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once training is completed, we can evaluate the agent's typical performance:"
      ],
      "metadata": {
        "id": "kANP2OEVW-t1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "Z-nZiiE-PpA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or more interestingly, we can take a look at a video of its performance.\n",
        "\n",
        "The below code will create a GIF of our **Lunar Lander** agent. When it's done, we can look at the GIF by (in Google Colab) clicking on the files tab on the left-hand side, and double-clicking on **lander.gif**."
      ],
      "metadata": {
        "id": "qDF-j8opXEaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "obs = model.env.reset()\n",
        "img = model.env.render()\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = model.env.step(action)\n",
        "    img = model.env.render()\n",
        "    if obs[0][-1] == 1 and obs[0][-2] == 1:\n",
        "      break\n",
        "\n",
        "imageio.mimsave(\"lander.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
      ],
      "metadata": {
        "id": "f3n024oMPrzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Other environments and algorithms\n",
        "\n",
        "There are a number of other environments provided by **gym**. There are also a number of other reinforcement learning algorithms provided by **Stable Baselines**.\n",
        "\n",
        "For example, let's look at the [**Cart Pole**](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) environment, and train an agent with the [A2C algorithm](https://stable-baselines3.readthedocs.io/en/v2.0.0/modules/a2c.html).\n",
        "\n",
        "\n",
        "\n",
        "![Cart Pole](https://www.gymlibrary.dev/_images/cart_pole.gif)"
      ],
      "metadata": {
        "id": "7LqhPaeIXmzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = A2C(\n",
        "    \"MlpPolicy\",\n",
        "    \"CartPole-v1\",\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "0BpIoZ0EYpnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model,\n",
        "    eval_env,\n",
        "    n_eval_episodes=10,\n",
        "    deterministic=True,\n",
        ")\n",
        "\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "o13nZFTGZdkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.learn(total_timesteps=int(1e4))"
      ],
      "metadata": {
        "id": "7t2YhJSnZfb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "id": "mdiSZVixZ2d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "obs = model.env.reset()\n",
        "img = model.env.render()\n",
        "for i in range(1000):\n",
        "    images.append(img)\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _ ,_ = model.env.step(action)\n",
        "    img = model.env.render()\n",
        "\n",
        "imageio.mimsave(\"cartpole.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=20)"
      ],
      "metadata": {
        "id": "rNS2-GgmZ5Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have time, you can try out some of the other **gym** environments, and some of the other reinforcement learning algorithms from **Stable Baselines**, using the above templates."
      ],
      "metadata": {
        "id": "QdOr_Z0oaY9V"
      }
    }
  ]
}