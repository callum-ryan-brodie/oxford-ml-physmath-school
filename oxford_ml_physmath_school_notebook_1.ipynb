{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9G7ygwLD4iibZFZCbi+PZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/callum-ryan-brodie/oxford-ml-physmath-school/blob/main/oxford_ml_physmath_school_notebook_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Jupyter notebooks and cloud\n",
        "\n",
        "(file extension .ipynb)\n",
        "\n",
        "they're interactive (like this one is)\n",
        "\n",
        "cells\n",
        "\n",
        "can run locally, by installing Jupyter Notebook\n",
        "\n",
        "Online platforms. Two of the most popular are [Google Colab](https://colab.research.google.com/) and [Kaggle notebooks](https://www.kaggle.com/docs/notebooks).\n",
        "\n",
        "## Deep learning frameworks: PyTorch vs Tensorflow\n",
        "\n",
        "The most popular frameworks for deep learning are PyTorch and TensorFlow. Notably, while in the past TensorFlow was dominant, more recently it has been rivaled in popularity by PyTorch. In particular, amongst published papers and their accompanying codebases, one sees that PyTorch is now the much more widely-used of the two, as shown in the following plots (taken from [this blog post](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/), which contains more details). This transition is slower outside of academia, but might reasonably be expected to follow suit with some time-lag.\n",
        "\n",
        "<img alt=\"PyTorch vs Tensorflow: papers in 2023\" width=\"500\" src=\"https://github.com/callum-ryan-brodie/oxford-ml-physmath-school/blob/main/images/assemblyai_papers_2023_plot.png?raw=1\"/>\n",
        "\n",
        "<img alt=\"PyTorch vs Tensorflow: repos in 2023\" width=\"500\" src=\"https://github.com/callum-ryan-brodie/oxford-ml-physmath-school/blob/main/images/assemblyai_repos_2023_plot.png?raw=1\"/>\n",
        "\n",
        "Probably the third most popular framework is JAX. In order of increasing complexity, these probably go TensorFlow, PyTorch, Jax."
      ],
      "metadata": {
        "id": "n6sxOIqHYckM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A_IWdSK69aL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Linear and logistic regression\n",
        "\n",
        "Single input: $x$\n",
        "\n",
        "Include the bias:\n",
        "$\\tilde{x} = (1,x)$\n",
        "\n",
        "Also useful then to define:\n",
        "$\\tilde{\\theta} = (1,\\theta)$\n",
        "\n",
        "Matrix with $\\tilde{x}^{(i)}$ as rows: $\\tilde{X}$"
      ],
      "metadata": {
        "id": "zwoI3zXZ-wu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "JgOi6tcK_wdO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression"
      ],
      "metadata": {
        "id": "3mKxVw_U9Unz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The cost function\n",
        "\n",
        "The classic cost function in linear regression is the sum-of-squares,\n",
        "$$\n",
        "J(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\Theta(x^{(i)})-y^{(i)})^2 \\,.\n",
        "$$\n",
        "\n",
        "$\\Theta X$\n",
        "\n",
        "(See for example Chapter 1 of Bishop for a deeper understanding of whether the sum-of-squares is a sensible cost function.)\n",
        "\n",
        "In the above notation, the predicted value is\n",
        "$$\n",
        "h_\\Theta(x^{(i)}) = 1 + \\theta \\cdot x^{(i)} \\equiv \\tilde{\\theta} \\cdot \\tilde{x}^{(i)}\n",
        "$$\n",
        "\n",
        "\n",
        "To compute , and the overall sum, you could use a set of `for` loops, but where possible it is typically much faster to use implementations of matrix and vector operations from a standard software\n",
        "libary such as NumPy. The syntax for some NumPy operations you might to use below are as follows.\n",
        "\n",
        "```python\n",
        "# import numpy as np\n",
        "\n",
        "# Matrix multiplication:\n",
        "np.dot(A,B)\n",
        "\n",
        "# Dot product between two vectors:\n",
        "np.inner(u,v)\n",
        "```"
      ],
      "metadata": {
        "id": "Dtr2-MBz92ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_lin(X, y, Theta):\n",
        "\n",
        "  m = len(y); # number of training examples\n",
        "\n",
        "  diffs = (np.dot(X,Theta)-y)\n",
        "  J = (1/(2*m)) * np.inner(diffs,diffs)\n",
        "\n",
        "  return J"
      ],
      "metadata": {
        "id": "aKLblOwh_JV9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient descent"
      ],
      "metadata": {
        "id": "PAo4zI6r9tUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "Zv4semuv9ylv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression"
      ],
      "metadata": {
        "id": "bSiMrx-K9XD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function\n",
        "\n",
        "\n",
        "(Again see for example Chapter 1 of Bishop for a deeper understanding of whether the above is a sensible cost function.)"
      ],
      "metadata": {
        "id": "YWqrKPGpF5Me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Building a neural network from scratch"
      ],
      "metadata": {
        "id": "VLmenlPGkP2R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCwAcZAH4ReE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sci8_H4Q7k_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4jVy71Va3meO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta =\n",
        "\n",
        "compute_cost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gdECafA6fuD",
        "outputId": "c7c4b8db-9ea4-4850-d3b2-f45d7db10c9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mat = [[1,2],[3,4]]\n",
        "test_vec = [1,2]\n",
        "\n",
        "np.dot(test_mat,test_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDY7fV9q4II2",
        "outputId": "ec55e206-c173-42f4-95cf-0270a48120e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretability of a neural network\n",
        "\n",
        "Check out [this video](https://youtu.be/aircAruvnKk) by 3Blue1Brown, in the specific context of handwritten digit recognition\n",
        "\n",
        "Check out Figure 2 of [this paper](https://arxiv.org/abs/1311.2901), in the broader context of image recognition"
      ],
      "metadata": {
        "id": "6RLeDaKZkto6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Utilising a software library: TensorFlow (/ PyTorch)"
      ],
      "metadata": {
        "id": "9_Mw6NfxkYbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fast.ai stuff\n",
        "\n",
        "\n",
        "The resnet networks were trained to output (the particular stuff involving nouns or whatever). But we pull off the last layer etc."
      ],
      "metadata": {
        "id": "sWt11MLGCYiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Deep learning with a CPU vs a GPU (vs a TPU)\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "If you're using Google Colab, you can switch between a CPU, a GPU, or a TPU by clicking 'Change runtime type' under 'Runtume' in the menu bar. You can experiment to see how the change affects the training of your neural network.\n",
        "\n",
        "Note that unless you pay for pay for Colab Pro / Pro+ (or pay individually for compute units), Google places a quota on how much GPU / TPU compute time you can use in a given time period. (The precise quotas aren't published, ostensibly because they tend to vary over time.)\n",
        "\n",
        "(You can also use a GPU or a TPU in a Kaggle notebook, by opening the sidebar (by clicking on the tiny arrow in the bottom right), and expanding 'Notebook options'.)\n",
        "\n",
        "### Ending your session\n",
        "\n",
        "If you're using Google Colab with a GPU / TPU, then to conserve your compute allowance you may want to make sure you end your session when you're done. This can be done either by clicking 'Disconnect and delete runtime' under 'Runtime' in the menu bar, or in a cell with the following code:\n",
        "\n",
        "```python\n",
        "from google.colab import runtime\n",
        "runtime.unassign()\n",
        "```"
      ],
      "metadata": {
        "id": "0_3J1jMcSxbn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hVwEtv1zb_Ry"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}